{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.9:4040\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1653642828563)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+------+\n",
      "|emp_id|name    |superior_emp_id|emp_dept_id|salary|\n",
      "+------+--------+---------------+-----------+------+\n",
      "|1     |Smith   |1              |10         |3000  |\n",
      "|1     |Smith   |1              |10         |3000  |\n",
      "|2     |Rose    |1              |20         |4000  |\n",
      "|3     |Williams|1              |10         |1000  |\n",
      "|4     |Jones   |2              |10         |2000  |\n",
      "|5     |Brown   |2              |40         |-1    |\n",
      "|6     |Brown   |2              |50         |-1    |\n",
      "+------+--------+---------------+-----------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emp: Seq[(Int, String, Int, String, Int)] = List((1,Smith,1,10,3000), (1,Smith,1,10,3000), (2,Rose,1,20,4000), (3,Williams,1,10,1000), (4,Jones,2,10,2000), (5,Brown,2,40,-1), (6,Brown,2,50,-1))\n",
       "empColumns: Seq[String] = List(emp_id, name, superior_emp_id, emp_dept_id, salary)\n",
       "import spark.sqlContext.implicits._\n",
       "datasetDF: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val dataset_df = spark.read.option(\"header\",\"true\").csv(\"Netflix.csv\")\n",
    "val emp = Seq((1,\"Smith\",1,\"10\",3000),\n",
    "              (1,\"Smith\",1,\"10\",3000),\n",
    "    (2,\"Rose\",1,\"20\",4000),\n",
    "    (3,\"Williams\",1,\"10\",1000),\n",
    "    (4,\"Jones\",2,\"10\",2000),\n",
    "    (5,\"Brown\",2,\"40\",-1),\n",
    "    (6,\"Brown\",2,\"50\",-1)\n",
    "  )\n",
    "val empColumns = Seq(\"emp_id\",\"name\",\"superior_emp_id\",\"emp_dept_id\",\"salary\")\n",
    "import spark.sqlContext.implicits._\n",
    "val datasetDF = emp.toDF(empColumns:_*)\n",
    "datasetDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+------+\n",
      "|emp_id|    name|superior_emp_id|emp_dept_id|salary|\n",
      "+------+--------+---------------+-----------+------+\n",
      "|     1|   Smith|              1|         10|  3000|\n",
      "|     1|   Smith|              1|         10|  3000|\n",
      "|     2|    Rose|              1|         20|  4000|\n",
      "|     3|Williams|              1|         10|  1000|\n",
      "|     4|   Jones|              2|         10|  2000|\n",
      "+------+--------+---------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----------+------+------+------+\n",
      "| name|superior_emp_id|emp_dept_id|salary|emp_id|emp_id|\n",
      "+-----+---------------+-----------+------+------+------+\n",
      "|Smith|              1|         10|  3000|     1|     1|\n",
      "|Smith|              1|         10|  3000|     1|     1|\n",
      "|Smith|              1|         10|  3000|     1|     1|\n",
      "|Smith|              1|         10|  3000|     1|     1|\n",
      "| Rose|              1|         20|  4000|     2|     2|\n",
      "+-----+---------------+-----------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "key_DF: Seq[String] = List(name, superior_emp_id, emp_dept_id, salary)\n",
       "duplicateJoinDF: org.apache.spark.sql.DataFrame = [name: string, superior_emp_id: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val key_DF = Seq(\"name\",\"superior_emp_id\",\"emp_dept_id\",\"salary\")\n",
    "\n",
    "val duplicateJoinDF = datasetDF.as(\"df1\").join(datasetDF.as(\"df2\"),\n",
    "    key_DF,\"left\")\n",
    "duplicateJoinDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable.ListBuffer\n",
       "list_single_colnames: scala.collection.mutable.ListBuffer[String] = ListBuffer(name, superior_emp_id, emp_dept_id, salary, emp_id, emp_id_x)\n",
       "duplicateJoinDF2: org.apache.spark.sql.DataFrame = [name: string, superior_emp_id: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "val list_single_colnames = ListBuffer[String]()\n",
    "for (colname <- duplicateJoinDF.columns.toSeq){\n",
    "    if (list_single_colnames.contains(colname)){\n",
    "        list_single_colnames += colname.concat(\"_x\") \n",
    "    }\n",
    "    else{\n",
    "        list_single_colnames += colname\n",
    "    }\n",
    "}\n",
    "    \n",
    "\n",
    "val duplicateJoinDF2 = duplicateJoinDF.toDF(list_single_colnames:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|emp_id|emp_id_x|count|\n",
      "+------+--------+-----+\n",
      "|     1|       1|    4|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "key: String = emp_id\n",
       "identifiedDuplicatesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [emp_id: int, emp_id_x: int ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val key = \"emp_id\"\n",
    "val identifiedDuplicatesDF = duplicateJoinDF2.groupBy(key,key + \"_x\").count()\n",
    "  .filter($\"count\" > 1)\n",
    "identifiedDuplicatesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cols: Seq[String] = WrappedArray(emp_id, emp_id_x, count)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cols = identifiedDuplicatesDF.columns.toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duplicatesOutputDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, superior_emp_id: int ... 7 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val duplicatesOutputDF = duplicateJoinDF2.as(\"df3\").join(identifiedDuplicatesDF.as(\"df4\"), col(\"df3.\" + key) === col(\"df4.\" + key + \"_x\")&&col(\"df3.\" + key) === col(\"df4.\" + key + \"_x\"),\"inner\").distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----------+------+------+--------+------+--------+-----+\n",
      "| name|superior_emp_id|emp_dept_id|salary|emp_id|emp_id_x|emp_id|emp_id_x|count|\n",
      "+-----+---------------+-----------+------+------+--------+------+--------+-----+\n",
      "|Smith|              1|         10|  3000|     1|       1|     1|       1|    4|\n",
      "+-----+---------------+-----------+------+------+--------+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicatesOutputDF.distinct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+------+\n",
      "|emp_id|    name|superior_emp_id|emp_dept_id|salary|\n",
      "+------+--------+---------------+-----------+------+\n",
      "|     1|   Smith|              1|         10|  3000|\n",
      "|     2|    Rose|              1|         20|  4000|\n",
      "|     3|Williams|              1|         10|  1000|\n",
      "|     4|   Jones|              2|         10|  2000|\n",
      "|     5|   Brown|              2|         40|    -1|\n",
      "|     6|   Brown|              2|         50|    -1|\n",
      "+------+--------+---------------+-----------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nonDuplicatesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [emp_id: int, name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nonDuplicatesDF = datasetDF.dropDuplicates(key_DF)\n",
    "nonDuplicatesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
